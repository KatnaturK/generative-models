{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_VAE(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8zVoSWby9z9"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "NUM_FEATURES = 16\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.0001\n",
        "device = 'cuda'"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c_ra8k30E5D"
      },
      "source": [
        "# class LinearVAE(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(LinearVAE, self).__init__()\n",
        "\n",
        "#     # encoder\n",
        "#     self.encoder = nn.Sequential(\n",
        "#         nn.Linear(in_features=784, out_features=512),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Linear(in_features=512, out_features=128),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Linear(in_features=128, out_features=NUM_FEATURES*2)\n",
        "#     )\n",
        "\n",
        "#     # decoder \n",
        "#     self.decoder = nn.Sequential(\n",
        "#       nn.Linear(in_features=NUM_FEATURES, out_features=128),\n",
        "#       nn.ReLU(),\n",
        "#       nn.Linear(in_features=128, out_features=512),\n",
        "#       nn.ReLU(),\n",
        "#       nn.Linear(in_features=512, out_features=784),\n",
        "#       nn.Sigmoid()\n",
        "#     )\n",
        "\n",
        "#   def reparameterize(self, mu, log_var):\n",
        "#     \"\"\"\n",
        "#     :param mu: mean from the encoder's latent space\n",
        "#     :param log_var: log variance from the encoder's latent space\n",
        "#     \"\"\"\n",
        "#     std = torch.exp(0.5*log_var) # standard deviation\n",
        "#     eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "#     sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "#     return sample\n",
        " \n",
        "#   def forward(self, x):\n",
        "#     # encoding\n",
        "#     x = self.encoder(x)\n",
        "#     x = x.view(-1, 2, NUM_FEATURES)\n",
        "#     # get `mu` and `log_var`\n",
        "#     mu = x[:, 0, :] # the first feature values as mean\n",
        "#     log_var = x[:, 1, :] # the other feature values as variance\n",
        "#     # get the latent vector through reparameterization\n",
        "#     z = self.reparameterize(mu, log_var)\n",
        "\n",
        "#     # decoding\n",
        "#     reconstruction = self.decoder(z)\n",
        "#     return reconstruction, mu, log_var\n",
        "\n",
        "#   def generate(self, sample):\n",
        "#     generated = self.decoder(sample)\n",
        "#     return generated\n",
        "\n",
        "# model = LinearVAE()\n",
        "# print(model)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58w4VG-kJVX6",
        "outputId": "a09c0cd2-467c-40b0-ec93-bc4046c766dc"
      },
      "source": [
        "class VAE(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(VAE, self).__init__()\r\n",
        "\r\n",
        "    # Encoder\r\n",
        "\r\n",
        "    self.conv0 = nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1)\r\n",
        "    #self.conv0_bn = nn.BatchNorm2d(32)\r\n",
        "    self.conv0_drop = nn.Dropout2d(0.25)\r\n",
        "    self.conv1 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "    #self.conv1_bn = nn.BatchNorm2d(64)\r\n",
        "    self.conv1_drop = nn.Dropout2d(0.25)\r\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "    #self.conv2_bn = nn.BatchNorm2d(128)\r\n",
        "    self.conv2_drop = nn.Dropout2d(0.25)\r\n",
        "    self.conv3 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\r\n",
        "    #self.conv3_bn = nn.BatchNorm2d(256)\r\n",
        "    self.conv3_drop = nn.Dropout2d(0.25)\r\n",
        "    self.fc = nn.Linear(12544, NUM_FEATURES*2)\r\n",
        "\r\n",
        "    # Decoder\r\n",
        "\r\n",
        "    self.fc1 = nn.Linear(16, 256*7*7)\r\n",
        "    self.trans_conv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\r\n",
        "    #self.trans_conv1_bn = nn.BatchNorm2d(128)\r\n",
        "    self.trans_conv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "    #self.trans_conv2_bn = nn.BatchNorm2d(64)\r\n",
        "    self.trans_conv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 1, padding = 1)\r\n",
        "    #self.trans_conv3_bn = nn.BatchNorm2d(32)\r\n",
        "    self.trans_conv4 = nn.ConvTranspose2d(32, 1, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\r\n",
        "\r\n",
        "  def reparameterize(self, mu, log_var):\r\n",
        "    \"\"\"\r\n",
        "    :param mu: mean from the encoder's latent space\r\n",
        "    :param log_var: log variance from the encoder's latent space\r\n",
        "    \"\"\"\r\n",
        "    std = torch.exp(0.5*log_var) # standard deviation\r\n",
        "    eps = torch.randn_like(std) # `randn_like` as we need the same size\r\n",
        "    sample = mu + (eps * std) # sampling as if coming from the input space\r\n",
        "    return sample\r\n",
        " \r\n",
        "  def forward(self, x):\r\n",
        "    # encoding\r\n",
        "    x = x.view(-1, 1, 28, 28)\r\n",
        "    x = F.leaky_relu(self.conv0(x), 0.2)\r\n",
        "    #x = self.conv0_bn(x)\r\n",
        "    x = self.conv0_drop(x)\r\n",
        "    x = F.leaky_relu(self.conv1(x), 0.2)\r\n",
        "    #x = self.conv1_bn(x)\r\n",
        "    x = self.conv1_drop(x)\r\n",
        "    x = F.leaky_relu(self.conv2(x), 0.2)\r\n",
        "    #x = self.conv2_bn(x)\r\n",
        "    x = self.conv2_drop(x)\r\n",
        "    x = F.leaky_relu(self.conv3(x), 0.2)\r\n",
        "    #x = self.conv3_bn(x)\r\n",
        "    x = self.conv3_drop(x)\r\n",
        "    x = x.view(-1, 12544)\r\n",
        "    x = self.fc(x)\r\n",
        "\r\n",
        "    x = x.view(-1, 2, NUM_FEATURES)\r\n",
        "    # get `mu` and `log_var`\r\n",
        "    mu = x[:, 0, :] # the first feature values as mean\r\n",
        "    log_var = x[:, 1, :] # the other feature values as variance\r\n",
        "    # get the latent vector through reparameterization\r\n",
        "    z = self.reparameterize(mu, log_var)\r\n",
        "\r\n",
        "    # decoding\r\n",
        "    x = self.fc1(z)\r\n",
        "    x = x.view(-1, 256, 7, 7)\r\n",
        "    x = F.relu(self.trans_conv1(x))\r\n",
        "    #x = self.trans_conv1_bn(x)\r\n",
        "    x = F.relu(self.trans_conv2(x))\r\n",
        "    #x = self.trans_conv2_bn(x)\r\n",
        "    x = F.relu(self.trans_conv3(x))\r\n",
        "    #x = self.trans_conv3_bn(x)\r\n",
        "    x = self.trans_conv4(x)\r\n",
        "    reconstruction = torch.sigmoid(x)\r\n",
        "    return reconstruction, mu, log_var\r\n",
        "\r\n",
        "  def generate(self, sample):\r\n",
        "    x = self.fc1(sample)\r\n",
        "    x = x.view(-1, 256, 7, 7)\r\n",
        "    x = F.relu(self.trans_conv1(x))\r\n",
        "    #x = self.trans_conv1_bn(x)\r\n",
        "    x = F.relu(self.trans_conv2(x))\r\n",
        "    #x = self.trans_conv2_bn(x)\r\n",
        "    x = F.relu(self.trans_conv3(x))\r\n",
        "    #x = self.trans_conv3_bn(x)\r\n",
        "    x = self.trans_conv4(x)\r\n",
        "    generated = torch.sigmoid(x)\r\n",
        "    return generated\r\n",
        "\r\n",
        "model = VAE()\r\n",
        "print(model)\r\n",
        "model = model.to(device)\r\n",
        "model.float()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VAE(\n",
            "  (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv0_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv1_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv3_drop): Dropout2d(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=12544, out_features=32, bias=True)\n",
            "  (fc1): Linear(in_features=16, out_features=12544, bias=True)\n",
            "  (trans_conv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (trans_conv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (trans_conv3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (trans_conv4): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (conv0_drop): Dropout2d(p=0.25, inplace=False)\n",
              "  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_drop): Dropout2d(p=0.25, inplace=False)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.25, inplace=False)\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (conv3_drop): Dropout2d(p=0.25, inplace=False)\n",
              "  (fc): Linear(in_features=12544, out_features=32, bias=True)\n",
              "  (fc1): Linear(in_features=16, out_features=12544, bias=True)\n",
              "  (trans_conv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  (trans_conv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (trans_conv3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (trans_conv4): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_BNxJOJ3Nhr"
      },
      "source": [
        "# transforms\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "])\n",
        "# train and validation data\n",
        "train_data = datasets.MNIST(\n",
        "  root='../input/data',\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=transform\n",
        ")\n",
        "val_data = datasets.MNIST(\n",
        "  root='../input/data',\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=transform\n",
        ")\n",
        "# train_data  = torch.utils.data.Subset(train_data, range(0, 10000-1))\n",
        "# val_data  = torch.utils.data.Subset(val_data, range(0, 2000-1))\n",
        "\n",
        "# training and validation data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  train_data,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  shuffle=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "  val_data,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  shuffle=False\n",
        ")\n",
        "\n",
        "normal_samples = torch.randn(25, NUM_FEATURES).to(device)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4FXsrm3X_G"
      },
      "source": [
        "def final_loss(bce_loss, mu, logvar):\n",
        "  \"\"\"\n",
        "  This function will add the reconstruction loss (BCELoss) and the \n",
        "  KL-Divergence.\n",
        "  KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "  :param bce_loss: recontruction loss\n",
        "  :param mu: the mean from the latent vector\n",
        "  :param logvar: log variance from the latent vector\n",
        "  \"\"\"\n",
        "  BCE = bce_loss \n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "  return BCE + KLD"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XwnSbCJ3wLu"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "\n",
        "def fit(model, dataloader):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(dataloader):\n",
        "    data, _ = data\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    reconstruction, mu, logvar = model(data)\n",
        "    bce_loss = criterion(reconstruction, data)\n",
        "    loss = final_loss(bce_loss, mu, logvar)\n",
        "    running_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss = running_loss/len(dataloader.dataset)\n",
        "  return train_loss"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ieQWT14NMQ"
      },
      "source": [
        "def validate(model, dataloader, samples):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader):\n",
        "      data, _ = data\n",
        "      data = data.to(device)\n",
        "      reconstruction, mu, logvar = model(data)\n",
        "      reconstruction = reconstruction.to(device)\n",
        "      bce_loss = criterion(reconstruction, data)\n",
        "      loss = final_loss(bce_loss, mu, logvar)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # save the last batch input and output of every epoch\n",
        "      if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
        "        num_rows = 8\n",
        "        samples.append(torch.cat((data.view(BATCH_SIZE, 1, 28, 28)[:num_rows], \n",
        "                           reconstruction.view(BATCH_SIZE, 1, 28, 28)[:num_rows])))\n",
        "\n",
        "  val_loss = running_loss/len(dataloader.dataset)\n",
        "  return val_loss"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7oDfJC2C08S"
      },
      "source": [
        "def view_samples(samples, epoch):\n",
        "  samples = samples.to('cpu')\n",
        "  fig, axes = plt.subplots(figsize=(5,5), nrows=5, ncols=5, sharex=True, sharey=True)\n",
        "  for ax, img in zip(axes.flatten(), samples):\n",
        "        img = img.detach()\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
        "  plt.savefig('graphs/CNN VAE Epoch ' + str(epoch) + '.png')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ccL5toX4Stc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ab7bd5-7ad9-4de3-f04e-708d60c2e8ab"
      },
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "samples = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_epoch_loss = fit(model, train_loader)\n",
        "    val_epoch_loss = validate(model, val_loader, samples)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    print('Epoch [{:5d}/{:5d}] | Train loss: {:6.4f} | Val loss: {:6.4f}'.format(\n",
        "                    epoch+1, NUM_EPOCHS, train_epoch_loss, val_epoch_loss))\n",
        "    if epoch % 5 == 0:\n",
        "      model.eval()\n",
        "      generated_images = model.generate(normal_samples)\n",
        "      generated_images = generated_images.view(25, 1, 28, 28)\n",
        "      view_samples(generated_images, epoch+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [    1/  100] | Train loss: 211.5640 | Val loss: 135.7545\n",
            "Epoch [    2/  100] | Train loss: 125.8046 | Val loss: 114.9909\n",
            "Epoch [    3/  100] | Train loss: 115.4649 | Val loss: 110.1024\n",
            "Epoch [    4/  100] | Train loss: 111.7206 | Val loss: 108.0703\n",
            "Epoch [    5/  100] | Train loss: 109.5002 | Val loss: 106.1999\n",
            "Epoch [    6/  100] | Train loss: 107.9087 | Val loss: 105.0341\n",
            "Epoch [    7/  100] | Train loss: 106.7520 | Val loss: 104.4775\n",
            "Epoch [    8/  100] | Train loss: 105.7674 | Val loss: 103.2803\n",
            "Epoch [    9/  100] | Train loss: 105.0521 | Val loss: 102.8292\n",
            "Epoch [   10/  100] | Train loss: 104.3404 | Val loss: 102.5800\n",
            "Epoch [   11/  100] | Train loss: 103.8254 | Val loss: 101.9558\n",
            "Epoch [   12/  100] | Train loss: 103.3672 | Val loss: 101.4623\n",
            "Epoch [   13/  100] | Train loss: 102.9385 | Val loss: 101.1669\n",
            "Epoch [   14/  100] | Train loss: 102.5843 | Val loss: 101.0965\n",
            "Epoch [   15/  100] | Train loss: 102.2410 | Val loss: 100.7495\n",
            "Epoch [   16/  100] | Train loss: 101.9121 | Val loss: 100.4075\n",
            "Epoch [   17/  100] | Train loss: 101.5828 | Val loss: 100.0107\n",
            "Epoch [   18/  100] | Train loss: 101.4041 | Val loss: 100.3482\n",
            "Epoch [   19/  100] | Train loss: 101.1228 | Val loss: 99.8239\n",
            "Epoch [   20/  100] | Train loss: 100.9292 | Val loss: 99.6146\n",
            "Epoch [   21/  100] | Train loss: 100.7144 | Val loss: 99.5541\n",
            "Epoch [   22/  100] | Train loss: 100.5243 | Val loss: 99.1987\n",
            "Epoch [   23/  100] | Train loss: 100.3621 | Val loss: 99.1461\n",
            "Epoch [   24/  100] | Train loss: 100.1529 | Val loss: 99.0250\n",
            "Epoch [   25/  100] | Train loss: 100.0328 | Val loss: 99.0015\n",
            "Epoch [   26/  100] | Train loss: 99.8754 | Val loss: 98.7080\n",
            "Epoch [   27/  100] | Train loss: 99.7303 | Val loss: 99.0638\n",
            "Epoch [   28/  100] | Train loss: 99.5864 | Val loss: 98.5541\n",
            "Epoch [   29/  100] | Train loss: 99.4638 | Val loss: 98.6384\n",
            "Epoch [   30/  100] | Train loss: 99.2991 | Val loss: 98.3629\n",
            "Epoch [   31/  100] | Train loss: 99.1972 | Val loss: 98.3533\n",
            "Epoch [   32/  100] | Train loss: 99.0745 | Val loss: 98.1968\n",
            "Epoch [   33/  100] | Train loss: 98.9603 | Val loss: 98.2232\n",
            "Epoch [   34/  100] | Train loss: 98.8924 | Val loss: 97.9522\n",
            "Epoch [   35/  100] | Train loss: 98.8358 | Val loss: 98.0063\n",
            "Epoch [   36/  100] | Train loss: 98.6937 | Val loss: 97.9457\n",
            "Epoch [   37/  100] | Train loss: 98.6087 | Val loss: 97.8454\n",
            "Epoch [   38/  100] | Train loss: 98.4896 | Val loss: 97.8333\n",
            "Epoch [   39/  100] | Train loss: 98.4249 | Val loss: 97.7703\n",
            "Epoch [   40/  100] | Train loss: 98.3574 | Val loss: 97.5725\n",
            "Epoch [   41/  100] | Train loss: 98.2396 | Val loss: 97.7156\n",
            "Epoch [   42/  100] | Train loss: 98.1964 | Val loss: 97.5839\n",
            "Epoch [   43/  100] | Train loss: 98.1205 | Val loss: 97.4882\n",
            "Epoch [   44/  100] | Train loss: 98.0100 | Val loss: 97.6027\n",
            "Epoch [   45/  100] | Train loss: 97.9539 | Val loss: 97.3954\n",
            "Epoch [   46/  100] | Train loss: 97.8939 | Val loss: 97.3149\n",
            "Epoch [   47/  100] | Train loss: 97.8071 | Val loss: 97.3971\n",
            "Epoch [   48/  100] | Train loss: 97.7711 | Val loss: 97.1912\n",
            "Epoch [   49/  100] | Train loss: 97.6645 | Val loss: 97.0658\n",
            "Epoch [   50/  100] | Train loss: 97.6186 | Val loss: 97.1010\n",
            "Epoch [   51/  100] | Train loss: 97.5437 | Val loss: 97.0467\n",
            "Epoch [   52/  100] | Train loss: 97.5053 | Val loss: 97.2456\n",
            "Epoch [   53/  100] | Train loss: 97.4608 | Val loss: 96.8972\n",
            "Epoch [   54/  100] | Train loss: 97.4084 | Val loss: 97.0668\n",
            "Epoch [   55/  100] | Train loss: 97.3541 | Val loss: 97.0876\n",
            "Epoch [   56/  100] | Train loss: 97.2459 | Val loss: 96.7775\n",
            "Epoch [   57/  100] | Train loss: 97.2285 | Val loss: 96.8870\n",
            "Epoch [   58/  100] | Train loss: 97.1502 | Val loss: 96.8109\n",
            "Epoch [   59/  100] | Train loss: 97.0831 | Val loss: 96.7381\n",
            "Epoch [   60/  100] | Train loss: 97.0432 | Val loss: 96.7678\n",
            "Epoch [   61/  100] | Train loss: 97.0242 | Val loss: 96.6177\n",
            "Epoch [   62/  100] | Train loss: 96.9698 | Val loss: 96.5405\n",
            "Epoch [   63/  100] | Train loss: 96.9359 | Val loss: 96.5968\n",
            "Epoch [   64/  100] | Train loss: 96.8854 | Val loss: 96.7751\n",
            "Epoch [   65/  100] | Train loss: 96.8154 | Val loss: 96.6981\n",
            "Epoch [   66/  100] | Train loss: 96.8002 | Val loss: 96.5142\n",
            "Epoch [   67/  100] | Train loss: 96.7358 | Val loss: 96.4380\n",
            "Epoch [   68/  100] | Train loss: 96.7205 | Val loss: 96.3823\n",
            "Epoch [   69/  100] | Train loss: 96.6499 | Val loss: 96.3954\n",
            "Epoch [   70/  100] | Train loss: 96.6107 | Val loss: 96.4007\n",
            "Epoch [   71/  100] | Train loss: 96.5889 | Val loss: 96.3798\n",
            "Epoch [   72/  100] | Train loss: 96.5201 | Val loss: 96.3262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rArOSlY_Rpw"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Val loss')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hypFOoCe_448"
      },
      "source": [
        "# rows = 5\n",
        "# fig, axes = plt.subplots(figsize=(7,12), nrows=2*rows, ncols=8, sharex=True, sharey=True)\n",
        "# flat_axes = [ax for ax_row in axes for ax in ax_row]\n",
        "\n",
        "# for row in range(rows):\n",
        "#   sample = samples[row * int(len(samples)/(rows))]\n",
        "#   view_samples(sample, flat_axes[row*16 : (row+1)*16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50jvoMFVAkSc"
      },
      "source": [
        "# sample_size = 12\n",
        "# indices = random.sample(range(0, 2000-1), sample_size)\n",
        "# sample_subset = torch.utils.data.Subset(val_data, random.sample(range(0, 2000-1), 12))\n",
        "# loader = torch.utils.data.DataLoader(sample_subset, batch_size=sample_size)\n",
        "# sample, _ = next(iter(loader))\n",
        "# print(sample.size())\n",
        "# sample = sample.view(sample.size(0), -1)\n",
        "# print(sample.size())\n",
        "\n",
        "# model.eval()\n",
        "# reconstruction, _, _ = model(sample)\n",
        "# print(reconstruction.size())\n",
        "\n",
        "# out = torch.cat((sample.view(sample_size, 1, 28, 28), \n",
        "#            reconstruction.view(sample_size, 1, 28, 28)))\n",
        "\n",
        "# print(out.size())\n",
        "\n",
        "# fig, axes = plt.subplots(figsize=(10,2), nrows=2, ncols=sample_size, sharex=True, sharey=True)\n",
        "# flat_axes = [ax for ax_row in axes for ax in ax_row]\n",
        "\n",
        "# view_samples(out, flat_axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_k31S_1T0V_"
      },
      "source": [
        "# model.eval()\n",
        "# generated_images = model.generate(normal_samples)\n",
        "# generated_images = generated_images.view(25, 1, 28, 28)\n",
        "\n",
        "# view_samples(generated_images, flat_axes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}